"""
structure_transformer
"""
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import math,copy,time
from torch.autograd import Variable
import matplotlib.pyplot as plt
import seaborn
from transformers.activations import get_activation
from typing import *
seaborn.set_context(context="talk") 


# define embedding of ridig and esm-seq
class Ridig_Embeddings(nn.Module):
    def __init__(self, d_model, d_esm_seq, n_rigid_type, n_rigid_property):
        super(Ridig_Embeddings, self).__init__()
        self.embed_rigid_type = nn.Embedding(n_rigid_type, d_model)
        self.embed_rigid_property = nn.Embedding(n_rigid_property, d_model)
        self.seq2d = nn.Linear(d_esm_seq, d_model) 
        self.mlp2d = nn.Sequential(
                  nn.Linear(d_model+d_model+d_model, d_model//2),
                  nn.ReLU(),
                  nn.Linear(d_model/2, d_model)
        )
        self.d_model = d_model

    def forward(
        self, 
        x_seq_esm: torch.Tensor, 
        x_rigid_type: torch.Tensor,
        x_rigid_proterty: torch.Tensor,
    ) -> torch.Tensor: 
        x_embed_rigid_type = self.embed_rigid_type(x_rigid_type)* math.sqrt(self.d_model) #[batch, L, 4, 512]
        x_embed_rigid_proterty = self.embed_rigid_property(x_rigid_proterty)* math.sqrt(self.d_model) #[batch, L, 4, 512]
        x_seq = self.seq2d(x_seq_esm) #[batch, L, 512]
        
        x_seq = x_seq.unsqueeze(-2)
        input = torch.cat([x_embed_rigid_type, x_embed_rigid_proterty,  x_seq.repeat(1, 1, 4, 1)], dim=-1)  #[batch, L, 4, 512*3]
        
        input_embed = self.mlp2d(input) #[batch, L, 4, 512]
        
        return input_embed
    
# define sinusoidal positional encoding 1
class SinusoidalPositionalEncoding(nn.Module):
    # Implement PE function
    def __init__(self, d_model, dropout, max_len =1000):
        super(SinusoidalPositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout) # dropout = 0.2 or 0.1
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1) # (1000) -> (1000,1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)) 
        pe[:, 0::2] = torch.sin(position * div_term) #
        pe[:, 1::2] = torch.cos(position * div_term) #
        pe = pe.unsqueeze(0) # (1000,512)->(1,1000,512) 
        self.register_buffer('pe', pe) # trainable parameters of the model
    
    def forward(self, x):
         x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)
         
         return self.dropout(x)
 
# define relative positional encoding 2
class RelativePositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout, max_len =1000):
        super(RelativePositionalEncoding, self).__init__()
        self.pe = nn.Embedding(max_len, d_model)
        self.rpe = nn.Parameter(torch.randn(d_model, d_model))
        #self.conv = nn.Conv1d(d_model, d_model, kernel_size=1, bias=False) #relative positional encoding based on conv
        self.dropout = nn.Dropout(dropout)
         
    def forward(self, x):
        seq_len = x.size(1)

        pos1 = torch.arange(seq_len).unsqueeze(1).expand(-1, seq_len)
        pos2 = torch.arange(seq_len).unsqueeze(0).expand(seq_len, -1)
        pos_diff = pos2 - pos1 #relative position
        
        rp_embedding = torch.zeros(seq_len, seq_len, x.size(-1), device=x.device)
        for k in range(x.size(-1)):
            rp_embedding[:, :, k] = torch.sin(pos_diff / 10000 ** (2 * k / x.size(-1)))

        rp_embedding = torch.matmul(rp_embedding, self.rpe) 
        
        
        x = x + self.pe.weight.unsqueeze(0).unsqueeze(0) + rp_embedding.unsqueeze(0)

        return self.dropout(x)
    
# define convolutional relative positional encoding 2        
class ConvRelativePositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout, max_len =1000):
        super(ConvRelativePositionalEncoding, self).__init__()
        self.pe = nn.Embedding(max_len, d_model)
        self.conv = nn.Conv1d(d_model, d_model, kernel_size=1, bias=False) #relative positional encoding based on conv
        self.dropout = nn.Dropout(dropout)
         
    def forward(self, x):
        seq_len = x.size(1)

        pos1 = torch.arange(seq_len).unsqueeze(1).expand(-1, seq_len)
        pos2 = torch.arange(seq_len).unsqueeze(0).expand(seq_len, -1)
        pos_diff = pos2 - pos1 #relative position
        
        rp_embedding = torch.zeros(seq_len, seq_len, x.size(-1), device=x.device)
        for k in range(x.size(-1)):
            rp_embedding[:, :, k] = torch.sin(pos_diff / 10000 ** (2 * k / x.size(-1)))

        rp_embedding = rp_embedding.transpose(1, 2).contiguous()
        rp_embedding = self.conv(rp_embedding) # relative positional encoding using conv
        
        x = x + self.pe.weight.unsqueeze(0).unsqueeze(0) + rp_embedding.unsqueeze(0)

        return self.dropout(x)       

# define ridig attention
class Rigid_Attention(nn.Module):
    def __init__(self, d_model):
        super(Rigid_MultiHeadAttention, self).__init__()
        self.d_model = d_model
        self.query = nn.Linear(d_model, d_model)
        self.key = nn.Linear(d_model, d_model)
        self.value = nn.Linear(d_model, d_model)
        self.value_3d = nn.Linear(d_model, 3)
        self.fc = nn.Linear(d_model, d_model)
    
    def forward(
        self, 
        x,
        seq_pair: torch.Tensor, # [batch, h, L, 1028] # pair?
        frame: torch.Tensor, #[batch,h,L*4,?]
        mask = None, 
        dropout=None,
    ):
        q = self.query(x)
        k = self.key(x)
        v = self.value(x)
        v_3d = self.value_3d(x)
        




# define the multi ridge attention
class Rigid_MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super(Rigid_MultiHeadAttention, self).__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        self.query = nn.Linear(d_model, d_model)
        self.key = nn.Linear(d_model, d_model)
        self.value = nn.Linear(d_model, d_model)
        self.value_3d = nn.Linear(d_model, 3)
        self.fc = nn.Linear(d_model, d_model)

    def forward(self, x, ):
        bsz = x.size(0)
        q = self.query(x).view(bsz, -1, self.n_heads, self.head_dim).transpose(1, 2)  # (bsz, n_heads, seq_len, head_dim)
        k = self.key(x).view(bsz, -1, self.n_heads, self.head_dim).transpose(1, 2)  # (bsz, n_heads, seq_len, head_dim)
        v = self.value(x).view(bsz, -1, self.n_heads, self.head_dim).transpose(1, 2)  # (bsz, n_heads, seq_len, head_dim)
        v_3d = self.value(x).view(bsz, -1, self.n_heads, self.head_dim).transpose(1, 2)
        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float))
        attn_weights = F.softmax(scores, dim=-1)

        attn_output = torch.matmul(attn_weights, v)  # (bsz, n_heads, seq_len, head_dim)
        attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, -1, self.d_model)  # (bsz, seq_len, d_model)
        attn_output = self.fc(attn_output)

        return attn_output


class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff):
        super(FeedForward, self).__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

class EncoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff):
        super(EncoderLayer, self).__init__()
        self.self_attn = Rigid_MultiHeadAttention(d_model, n_heads)
        self.ff = FeedForward(d_model, d_ff)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        # Self-attention
        residual = x
        x = self.norm1(x)
        x = self.self_attn(x)
        x = self.dropout(x)
        x = residual + x

        # Feed forward
        residual = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.dropout(x)
        x = residual + x

        return x

class AnglesPredictor(nn.Module):
    def __init__(
        self,
        d_model: int,
        d_out: int = 4,
        activation: Union[str, nn.Module] = "gelu",
        eps: float = 1e-12,
    ) -> None:
        super().__init__()
        self.d_model = d_model
        self.d_out = d_out
        self.dense1 = nn.Linear(d_model, d_model)

        if isinstance(activation, str):
            self.dense1_act = get_activation(activation)
        else:
            self.dense1_act = activation()
        self.layer_norm = nn.LayerNorm(d_model, eps=eps)

        self.dense2 = nn.Linear(d_model, d_out)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.dense1(x)
        x = self.dense1_act(x)
        x = self.layer_norm(x)
        x = self.dense2(x)
        return x



class Ridge_Transformer(nn.Module):
    def __init__(self,
        d_model: int = 512,
        d_esm_seq: int = 1024,
        n_rigid_type: int = 4,
        n_rigid_property: int = 19,
        n_layers: int = 4,
        n_heads: int = 8,
        d_ff: int = 1024, #hidden layer dim
        d_angles:int =4,
        max_seq_len: int =5000,
        dropout: float = 0.1,
    ):
        super(Ridge_Transformer, self).__init__()
        self.embedding = Ridig_Embeddings(d_model, d_esm_seq, n_rigid_type, n_rigid_property)
        self.pos_encoding = SinusoidalPositionalEncoding(d_model, dropout, max_seq_len)
        self.linear = nn.Linear(d_esm_seq, d_model)
        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)])
        self.predict_angles = AnglesPredictor(d_model, d_angles)
        self.norm = nn.LayerNorm(d_model)

    def forward(self,
        angels: torch.Tensor, #[batch,128,4]
        backbone_coords: torch.Tensor, #[batch,128,4,3]
        time: torch.Tensor, 
        attention_mask: torch.Tensor,
        x_seq_esm: torch.Tensor,  #[batch,128,1024]
        x_rigid_type: torch.Tensor, #[batch,128,4,4] x_rigid_type[-1]=one hot
        x_rigid_proterty: torch.Tensor, #[batch,128,4,19]
    ):
        x_rigid = self.embedding(x_seq_esm, x_rigid_type, x_rigid_proterty) # [batch,128,4,512]
        x_rigid = x_rigid.view(x_rigid.size(0), -1, x_rigid.size(-1)) # [batch,128*4,512]
        x_rigid = self.pos_encoding(x_rigid) #rigid finish # [batch,128*4,512]
        

        #calculate correlation matrix based on 
        x_seq_esm = self.linear(x_seq_esm)  # [batch,128,512]
        seq_correlation_matrix= calculate_correlation_matrix(x_seq_esm) # [batch,128,batch,128] ? 
        
        for layer in self.layers:
            sidechain_frame = cal_sidechain_frame(angels, backbone_coords) # [batch,128,4,3,?] 
            #r = sidechain_frame[0] r_ba = sidechain_frame[1]
            # O = sidechain_frame[2]
            x_rigid = layer(x_rigid, sidechain_frame, seq_correlation_matrix, attention_mask) # [batch,128, 4,512]
            x_rigid = self.norm(x_rigid)
            x_rigid = x_rigid+time
            angels = self.predict_angles(x_rigid) #[batch,128,4]
        
        return angels









'''         
def rigid_attention(
    query: torch.Tensor, #[batch, h, L*4, 512]
    key: torch.Tensor, #[batch, h, L*4, 512]
    value: torch.Tensor, #[batch, h, L*4, 512]
    value_3d: torch.Tensor, #[batch, h, L*4, 3]
    seq_pair: torch.Tensor, # [batch, h, L, 1028] # pair?
    frame: torch.Tensor, #[batch,h,L*4,?]
    mask = None, 
    dropout=None,
) -> torch.Tensor:
    d_k = query.size(-1)
   # scores = (torch.matmul(query, key.transpose(-2, -1))+seq_pair)/ math.sqrt(d_k) ##[batch, h, L*4, L*4]
    scores = (torch.matmul(query, key.transpose(-2, -1))+seq_pair)/ math.sqrt(d_k) ##[batch, h, L*4, L*4]

    if mask is not None:
       scores = scores.masked_fill(mask == 0, -1e9) 
    
    p_attn = F.softmax(scores, dim = -1)
    
    if  dropout is not None:
        p_attn = dropout(p_attn) 
    
    O_v = torch.matmul(p_attn,value) #error
    O_3d = torch.matmul(p_attn,frame) #error
    O_seq = torch.matmul(p_attn, seq_pair) #error
    
    return torch.cat([O_v, O_3d,  O_seq], dim=-1)#error
''' 
'''
class MultiHeadedAttention(nn.Module): 
    def __init__(self, h, d_model, dropout=0.1):
        super(MultiHeadedAttention, self).__init__() 
        #assert d_model % h == 0
        self.d_k = d_model // h
        self.h = h
        self.linears = clones(nn.Linear(d_model, d_model),4)
        self.value_3d = nn.Linear(d_model, 3)
        #self.attn = None
        self.dropout = nn.Dropout(dropout)
    def forward(self, query, key, value, mask=None):
        if mask is not None:
            mask = mask.unsqueeze(1)          
        nbatches = query.size(0)
        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k)
                             .transpose(1, 2) for l, x in zip(self.linears, (query, key, value))] 
         
        x = rigid_attention(query, key, value, mask=mask, dropout=self.dropout)
        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)
        
        return self.linears[-1](x)

class EncoderLayer(nn.Module):
    def __init__(self, n_heads, d_model, d_ff, dropout=0.1):
        super(EncoderLayer, self).__init__()
        self.multihead_attention = MultiHeadedAttention(n_heads, d_model, dropout)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout),
        )
        self.layer_norm1 = nn.LayerNorm(d_model)
        self.layer_norm2 = nn.LayerNorm(d_model)
        
    def forward(self, x, mask=None):
        attn_output = self.multihead_attention(x, x, x, mask=mask)
        attn_output = self.layer_norm1(x + attn_output)
        ffn_output = self.feed_forward(attn_output)
        ffn_output = self.layer_norm2(attn_output + ffn_output)
        return ffn_output 
         
class Encoder(nn.Module):
    def __init__(self, n_layers, n_heads, d_model, d_ff, dropout=0.1):
        super().__init__()

        self.layers = nn.ModuleList([
            EncoderLayer(n_heads, d_model, d_ff, dropout)
            for _ in range(n_layers)
        ])       
    def forward(self, x, mask=None):
        for layer in self.layers:
            x = layer(x, mask=mask)
        return x
       
'''